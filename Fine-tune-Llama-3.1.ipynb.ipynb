{"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95755,"sourceType":"modelInstanceVersion","modelInstanceId":80314,"modelId":104735}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Llama 3 and Using It Locally\n\nhttps://www.datacamp.com/tutorial/llama3-fine-tuning-locally","metadata":{"id":"1Q75oW9vxlES"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"klbqhHEwGGSa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U transformers\n%pip install -U datasets\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install -U bitsandbytes\n%pip install -U wandb","metadata":{"id":"bQHZshF0xjgb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"id":"jXtiG8Rgxzhh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN_LLAMA3.1\")\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset',\n    job_type=\"training\",\n    anonymous=\"allow\"\n)","metadata":{"id":"NWJge-Zhx1uw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_dtype         = torch.float16\nattn_implementation = \"eager\"","metadata":{"id":"wlESt4hkx7R5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model   = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\ndataset_name = 'Adun/katunyou01'\n\n#save model to gdrive\nlora_model   = \"/kaggle/working/TrainModel/Meta-Llama-3.1-8B-Instruct-katunyou01\"","metadata":{"id":"_xaIDKZCx44h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer        = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"id":"-j4lvWqux9ia","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"id":"zwJSB4E2yCPJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\n#dataset = dataset.shuffle(seed=65).select()\ndataset = dataset.shuffle(seed=42).select(range(247))    # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"input\"]},\n               {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"id":"Ig4SSI4yyEca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)\ndataset","metadata":{"id":"8O9CdudnyGzZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=lora_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True\n)\n# report_to=\"wandb\"","metadata":{"id":"MlZvo9nDyJHS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"id":"4uP9sr4QyLUZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"Y_vQwo4nyNcR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"eu8yZEgSyRkH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"คุณเป็นใคร\"\n    }\n]\n\nprompt  = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n\ninputs  = tokenizer(prompt, return_tensors='pt', padding=True,truncation=True).to(\"cuda\")\noutputs = model.generate(**inputs, max_length=512,num_return_sequences=1)\n\ntext    = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])","metadata":{"id":"vqCPbU5tyU85","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(lora_model)\n# trainer.model.push_to_hub(lora_model, use_temp_dir=False)","metadata":{"id":"0BnZ_HoJyVqR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/Meta-Llama-3.1-8B-Instruct-katunyou01.zip /kaggle/working/TrainModel/Meta-Llama-3.1-8B-Instruct-katunyou01","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"download the model","metadata":{}},{"cell_type":"markdown","source":"# Merging Llama3 + Lora","metadata":{"id":"6iJY9xssyc2D"}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"id":"qU2Rj0mVyYOX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model  = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\nlora_model  = \"/kaggle/input/llama3.1-lora-adaptor-katunyouai/transformers/default/1/checkpoint-1110\"","metadata":{"id":"8-u1wDVrykQw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN_LLAMA3.1\")\nlogin(token = hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom trl import setup_chat_format\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n\ntorch_dtype         = torch.float16\nattn_implementation = \"eager\"\n\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nprint(\"load base model\")\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n#         quantization_config=bnb_config,\n)\n\nprint(\"setup chat format\")\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nprint(\"load lora\")\nmodel = PeftModel.from_pretrained(base_model_reload, lora_model)\nprint(\"merge\")\nmodel = model.merge_and_unload()","metadata":{"id":"GSCuxrMfymiR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(\"JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\")\n# tokenizer.save_pretrained(\"JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\")","metadata":{"id":"ijpWAFBYyqpR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import login\n# login(token=user_secrets.get_secret(\"HF_TOKEN\"))\n# model.push_to_hub(\"JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\")\n# tokenizer.push_to_hub(\"JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\")","metadata":{"id":"51n-fONtyuEB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\nfrom huggingface_hub import login\n\nlogin(token=user_secrets.get_secret(\"HF_TOKEN\"))\n\napi = HfApi()\napi.upload_folder(folder_path=\"/kaggle/working/JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\", repo_id=\"JuniorThap/Meta-Llama-3.1-8B-Instruct-QLoRA-KatuntyouAI-v1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"คุณคือ?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipe   = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipe(prompt, max_new_tokens=128, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])","metadata":{"id":"UskgS4X9yoio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting the HF Model to Llama.cpp GGUF","metadata":{"id":"GiaPPYxpyxt5"}},{"cell_type":"code","source":"%cd /content\n!git clone https://github.com/ggerganov/llama.cpp","metadata":{"id":"rwUgpaWECItQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert GGUF FP16","metadata":{"id":"o3tklesRalto"}},{"cell_type":"code","source":"!python llama.cpp/convert_hf_to_gguf.py /content/drive/MyDrive/BaseModel/llama-3-typhoon-v1.5x-8b-instruct \\\n  --outfile /content/drive/MyDrive/BaseModel/llama-3-typhoon-v1.5x-8b-instruct/llama-3-typhoon-v1.5x-8b-instruct-gguf-fp16.gguf \\\n  --outtype f16","metadata":{"id":"l-ntMcHqakmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert GGUF Q8_0","metadata":{"id":"VzCKhFatasoF"}},{"cell_type":"code","source":"!python llama.cpp/convert_hf_to_gguf.py https://huggingface.co/scb10x/llama-3-typhoon-v1.5x-8b-instruct \\\n  --outfile llama-3-typhoon-v1.5x-8b-instruct-gguf-q8.gguf \\\n  --outtype q8_0","metadata":{"id":"SioclHRjCIig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /content\n# !git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 make -j > /dev/null","metadata":{"id":"cDuqTGisyu2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python convert.py /kaggle/input/fine-tuned-adapter-to-full-model/llama-3-8b-instruct-pantip/ \\\n    --outfile /kaggle/working/llama-3-8b-instruct-pantip-f16.gguf \\\n    --outtype f16 \\\n    --vocab-type bpe","metadata":{"id":"vnSyJWZky1i5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantizing the GGUF Q4 model","metadata":{"id":"6-l0RYFAy55g"}},{"cell_type":"code","source":"!/content/llama.cpp/llama-quantize \\\n/content/drive/MyDrive/BaseModel/llama-3-typhoon-v1.5x-70b-instruct/llama-3-typhoon-v1.5x-70b-instruct-gguf-fp16.gguf \\\nllama-3-typhoon1.5x-70-instruct-Q4_K_M.gguf \\\nQ4_K_M","metadata":{"id":"4coOlHqctF6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working/\n# !./llama.cpp/quantize /kaggle/input/hf-llm-to-gguf/llama-3-8b-chat-doctor.gguf llama-3-8b-chat-doctor-Q4_K_M.gguf Q4_K_M","metadata":{"id":"rAm064_7y9BZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Push to Huggingface","metadata":{"id":"EPWrOZz2CjYf"}},{"cell_type":"code","source":"from huggingface_hub import login, HfApi\n\nhf_token     = \"\"\nlogin(token = hf_token)\n\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"llama-3-typhoon-v1.5x-8b-instruct-pantip\",\n    path_in_repo=\"llama-3-typhoon-v1.5x-8b-instruct-pantip-Q4_K_M.gguf\",\n    repo_id=\"Adun/llama-3-typhoon-v1.5x-8b-instruct-gguf\",\n    repo_type=\"model\",\n)\n","metadata":{"id":"dWj-rIPcy-15"},"execution_count":null,"outputs":[]}]}